{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crash Course in PyTorch\n",
    "\n",
    "### Part 0 - Introduction\n",
    "\n",
    "#### Instructor: Tudor Cebere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python library used for high performance tensor operations.\n",
    "\n",
    "PyTorch internals are written in C++ for memory-friendly operations and it exposes a set of primitive routines to a high level API in Python, due to its ease of use and beginner-friendly syntax. PyTorch exposes its API to C++, rust <3 , Haskell, swift and it can be compiled to other frameworks as well.\n",
    "\n",
    "It's being used mainly in Python due to its ease of prototyping and clean syntax, the computationally heavy operations being forwarded to the C++ core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Tensor?\n",
    "\n",
    "We can look at a Tensor as a multidimensional matrix or a generalization of a vector. Let's try to define  what the tensor behavior should be from a computer science perspective. Let's imagine that you have a tensor `v`. What will you get if you try to get the first element of a tensor:\n",
    "* If `v` is a 4D array, the requested element will be a 3D tensor.\n",
    "* If `v` is a 3D array, the requested element will be a 2D tensor (or a matrix).\n",
    "* If `v` is a 2D array, the requested element will be a 1D tensor (or a vector).\n",
    "* If `v` is a 1D array, the requested element will be a scalar.\n",
    "* If `v` is a scalar, the request element will raise an error.\n",
    "\n",
    "In other words, let's imagine the dimensions of a tensor:\n",
    "\n",
    "![](https://miro.medium.com/max/2088/1*TPauIPgMOuwowxd53zNKVA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the notebook cells are stateful, you need to run this once per session.\n",
    "import torch as th\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the int types are converted to float. Why?\n",
    "\n",
    "one_dimension_tensor = th.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Lists in python have no restriction on their internal types and almost all numerical types in python\n",
    "# can be converted to float.\n",
    "\n",
    "another_one_dimensional_int_tensor = th.Tensor([False, True, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When creating a tensor, you can specify the data type (dtype) you cast the data to. \n",
    "\n",
    "a = th.ones((2, 3), dtype=th.int)\n",
    "print(a)\n",
    "\n",
    "mask = th.zeros((2, 3, 4), dtype=th.bool)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The underlying data type can be specified by running custom Tensors as well, for\n",
    "# example: LongTensor\n",
    "\n",
    "v = th.LongTensor([1,2,3])   # A Tensor of type Long\n",
    "print(f\"LongTensor: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize each element with a random number sampled from the uniform distribution.\n",
    "uniform = th.rand(2, 3)\n",
    "print(f\"From the uniform distribution:\\n{uniform}\\n\")\n",
    "\n",
    "# Initialize each element with a random number sampled from the normal distribution.\n",
    "normal = th.randn(2, 3)\n",
    "print(f\"From the normal distribution:\\n{normal}\\n\")\n",
    "\n",
    "# Initialize each element with a permutation from a range.\n",
    "perm = th.randperm(4)\n",
    "print(f\"From permutations:\\n{perm}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.linspace(1, 10, steps=10) # Create a Tensor with 10 linearly-spaced points from the interval [1, 10]\n",
    "v = torch.logspace(start=-10, end=10, steps=5) # Size 5: 1.0e-10 1.0e-05 1.0e+00, 1.0e+05, 1.0e+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: If you are familiar with NumPy, the indexing is similar!\n",
    "\n",
    "x = th.Tensor(2, 3)  # An un-initialized Tensor object. x holds random garbage data.\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the first array\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the second element from the first array\n",
    "print(x[0][1])\n",
    "\n",
    "# similar to:\n",
    "print(x[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .size() gives you good hints about how you can do the indexing!\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .numel() gives you information about the number of elements in the tensor (this can become huge)\n",
    "\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Swiss knife when it comes to tensor shaping is view:\n",
    "x = th.randn(2, 3)\n",
    "\n",
    "# lets see our tensor unrolled\n",
    "y = x.view(6)\n",
    "\n",
    "# what does -1 on a tensor dimension represent when viewing the tensor?\n",
    "z = x.view(-1, 2)\n",
    "\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.arange(9)\n",
    "v = v.view(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "torch.cat((x, x, x), 0)          # Concatenate on the 0 dimension\n",
    "\n",
    "# Stack\n",
    "r = torch.stack((v, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index select\n",
    "# 0 2\n",
    "# 3 5\n",
    "# 6 8\n",
    "indices = torch.LongTensor([0, 2])\n",
    "r = torch.index_select(v, 1, indices) # Select element 0 and 2 for each dimension 1.\n",
    "\n",
    "# Masked select\n",
    "# 0  0  0\n",
    "# 1  1  1\n",
    "# 1  1  1\n",
    "mask = v.ge(3)\n",
    "\n",
    "# Size 6: 3 4 5 6 7 8\n",
    "r = torch.masked_select(v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.ones(2,1,2,1) # Size 2x1x2x1\n",
    "r = torch.squeeze(t)     # Size 2x2\n",
    "r = torch.squeeze(t, 1)  # Squeeze dimension 1: Size 2x2x1\n",
    "# Note: you can only squeeze dimensions of length 1!\n",
    "\n",
    "# Un-squeeze a dimension\n",
    "x = torch.Tensor([1, 2, 3])\n",
    "r = torch.unsqueeze(x, 0)       # Size: 1x3\n",
    "r = torch.unsqueeze(x, 1)       # Size: 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose dim 0 and 1\n",
    "r = torch.transpose(v, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Math operations\n",
    "f= torch.FloatTensor([-1, -2, 3])\n",
    "r = torch.abs(f)      # 1 2 3\n",
    "\n",
    "# Add x, y and scalar 10 to all elements\n",
    "r = torch.add(x, 10)\n",
    "r = torch.add(x, 10, y)\n",
    "\n",
    "# Clamp the value of a Tensor\n",
    "r = torch.clamp(v, min=-0.5, max=0.5)\n",
    "\n",
    "# Element-wise divide\n",
    "r = torch.div(v, v+0.03)\n",
    "\n",
    "# Element-wise multiple\n",
    "r = torch.mul(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison\n",
    "# Size 3x3: Element-wise comparison\n",
    "r = torch.eq(v, v)\n",
    "\n",
    "# Max element with corresponding index\n",
    "r = torch.max(v, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.randn(2, 4)\n",
    "vec = torch.randn(4)\n",
    "r = torch.mv(mat, vec)\n",
    "\n",
    "\n",
    "M = torch.randn(2)\n",
    "mat = torch.randn(2, 3)\n",
    "vec = torch.randn(3)\n",
    "r = torch.addmv(M, mat, vec)\n",
    "\n",
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 4)\n",
    "r = torch.mm(mat1, mat2)\n",
    "\n",
    "M = torch.randn(3, 4)\n",
    "mat1 = torch.randn(3, 2)\n",
    "mat2 = torch.randn(2, 4)\n",
    "r = torch.addmm(M, mat1, mat2)\n",
    "\n",
    "v1 = torch.arange(1, 4)    # Size 3\n",
    "v2 = torch.arange(1, 3)    # Size 2\n",
    "r = torch.ger(v1, v2)\n",
    "\n",
    "vec1 = torch.arange(1, 4)  # Size 3\n",
    "vec2 = torch.arange(1, 3)  # Size 2\n",
    "M = torch.zeros(3, 2)\n",
    "r = torch.addr(M, vec1, vec2)\n",
    "\n",
    "batch1 = torch.randn(10, 3, 4)\n",
    "batch2 = torch.randn(10, 4, 5)\n",
    "r = torch.bmm(batch1, batch2)\n",
    "\n",
    "M = torch.randn(3, 2)\n",
    "batch1 = torch.randn(5, 3, 4)\n",
    "batch2 = torch.randn(5, 4, 2)\n",
    "r = torch.addbmm(M, batch1, batch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More cool stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.diag(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
