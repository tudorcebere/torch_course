{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crash Course in PyTorch\n",
    "\n",
    "### Part 0 - Introduction\n",
    "\n",
    "#### Instructor: Tudor Cebere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.ambient-it.net/wp-content/uploads/2018/07/Logo-PyTorch-200x175.jpg)\n",
    "\n",
    "PyTorch is a Python library used for high performance tensor operations.\n",
    "\n",
    "PyTorch internals are written in C++ for memory-friendly operations and it exposes a set of primitive routines to a high level API in Python, due to it's ease of use and beginner friendly syntax. PyTorch exposes it's API to C++, rust <3 , Haskell, swift and it can be compiled to other frameworks as well.\n",
    "\n",
    "It's being used mainly in Python due to it's ease of prototyping and clean syntax, the computational heave operations being forwarded to the C++ core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Tensor?\n",
    "\n",
    "We can look at a Tensor as a multidimensional matrix or a generalization of a vector. Let's try to define  what the tensor behavior should be from a computer scientist. Let's imagine that you have a tensor `v`. What will you get if you try to get the first element of a tensor:\n",
    "* If `v` is a 4D array, the requested element will be a 3D tensor.\n",
    "* If `v` is a 3D array, the requested element will be a 2D tensor (or a matrix).\n",
    "* If `v` is a 2D array, the requested element will be a 1D tensor (or a vector).\n",
    "* If `v` is a 1D array, the requested element will be a scalar.\n",
    "* If `v` is a scalar, the request element will raise an error.\n",
    "\n",
    "In other words, let's imagine the dimensions of a tensor:\n",
    "\n",
    "![](https://miro.medium.com/max/2088/1*TPauIPgMOuwowxd53zNKVA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the notebook cells are stateful, you need to run this once per session.\n",
    "import torch as th\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Tensor\n",
    "\n",
    "The starting point:\n",
    "[Tensor Creation API](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "![](https://i.imgflip.com/4mrg2n.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: note the fact that the int types are converted to float. Why?\n",
    "one_dimension_tensor = th.Tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Lists in python have no restriction on their internal types and almost all numerical types in python\n",
    "# can be converted to float.\n",
    "\n",
    "another_one_dimensional_int_tensor = th.Tensor([False, True, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: When creating a tensor, you can specify the dtype on which to cast the data. \n",
    "\n",
    "a = th.ones((2, 3), dtype=th.int)\n",
    "print(a)\n",
    "\n",
    "mask = th.zeros((2, 3, 4), dtype=th.bool)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The underlying data type can be specified by running custom Tensors as well, for\n",
    "# example: LongTensor\n",
    "\n",
    "v = th.LongTensor([1,2,3])   # A Tensor of type Long\n",
    "print(f\"LongTensor: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize each element with a random number sampled from the uniform distribution.\n",
    "uniform = th.rand(2, 3)\n",
    "print(f\"From the uniform distribution:\\n{uniform}\\n\")\n",
    "\n",
    "# Initialize each element with a random number sampled from the normal distribution.\n",
    "normal = th.randn(2, 3)\n",
    "print(f\"From the normal distribution:\\n{normal}\\n\")\n",
    "\n",
    "# Initialize each element with a permutation from a range.\n",
    "perm = th.randperm(4)\n",
    "print(f\"From permutations:\\n{perm}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = th.linspace(1, 10, steps=10) # Create a Tensor with 10 linear points for (1, 10) inclusively\n",
    "v = th.logspace(start=-10, end=10, steps=5) # Size 5: 1.0e-10 1.0e-05 1.0e+00, 1.0e+05, 1.0e+10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor indexing\n",
    "\n",
    "![](https://miro.medium.com/max/1276/1*WArDf9h6Dtbo-4H5P4lguQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip: If you are familiar with NumPy, the indexing is similar!\n",
    "x = th.randn(2, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the first array\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get the second element from the first array\n",
    "print(x[0][1])\n",
    "\n",
    "# similar to:\n",
    "print(x[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .size() gives you good hints about how you can do the indexing!\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .numel() gives you information about the numer of elements in the tensor (this can become huge)\n",
    "print(x.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your Swiss knife when it comes to tensor shaping is view:\n",
    "x = th.randn(2, 3)\n",
    "\n",
    "# lets ssee our tensor unrolled\n",
    "y = x.view(6)\n",
    "\n",
    "# what a -1 on a tensor dimension mean when viewing the tensor?\n",
    "z = x.view(-1, 2)\n",
    "\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = th.arange(9)\n",
    "print(v)\n",
    "v = v.view(3, 3)\n",
    "print(v)\n",
    "print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation\n",
    "x3 = th.cat((x, x, x), 0) # Concatenate in the 0 dimension\n",
    "print(x3)\n",
    "\n",
    "# Stack\n",
    "r = th.stack((v, v))\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index select\n",
    "# 0 2\n",
    "# 3 5\n",
    "# 6 8\n",
    "\n",
    "indices = th.LongTensor([0, 2])\n",
    "r = th.index_select(v, 1, indices) # Select element 0 and 2 for each dimension 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masked select\n",
    "# 0  0  0\n",
    "# 1  1  1\n",
    "# 1  1  1\n",
    "mask = v.ge(3)\n",
    "\n",
    "# Size 6: 3 4 5 6 7 8\n",
    "r = th.masked_select(v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = th.ones(2,1,2,1) # Size 2x1x2x1\n",
    "print(t)\n",
    "r = th.squeeze(t)     # Size 2x2\n",
    "print(r)\n",
    "r = th.squeeze(t, 1)  # Squeeze dimension 1: Size 2x2x1\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-squeeze a dimension\n",
    "x = th.Tensor([1, 2, 3])\n",
    "print(x)\n",
    "r = th.unsqueeze(x, 0)       # Size: 1x3\n",
    "print(r)\n",
    "r = th.unsqueeze(x, 1)       # Size: 3x1\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose dim 0 and 1\n",
    "print(v)\n",
    "\n",
    "transposed_1 = th.transpose(v, 0, 1)\n",
    "print(transposed_1)\n",
    "\n",
    "transposed_2 = v.T\n",
    "print(transposed_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_data = th.tensor([-1, 2, -3])\n",
    "x = th.abs(neg_data) # 1 2 3\n",
    "y = th.tensor([-1, -1, -1])\n",
    "\n",
    "# Add x, y and scalar 10 to all elements\n",
    "r = th.add(x, 10) # 11 12 13\n",
    "print(r)\n",
    "\n",
    "# What the 3 arguments does?\n",
    "r = th.add(x, 10, y) # 10, 11, 12\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp the value of a Tensor\n",
    "r = th.clamp(v, min=-0.5, max=0.5)\n",
    "\n",
    "# Element-wise divide\n",
    "r = th.div(v, v+0.03)\n",
    "\n",
    "# Element-wise multiple\n",
    "r = th.mul(v, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparison\n",
    "# Size 3x3: Element-wise comparison\n",
    "r = th.eq(v, v)\n",
    "\n",
    "# Max element with corresponding index\n",
    "r = th.max(v, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What will the following snippet of code return?\n",
    "result = th.tensor([1, 2, 3]) == th.tensor([1, 4, 6])\n",
    "print(result)\n",
    "\n",
    "print(result.all())\n",
    "\n",
    "print(result.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm is matrix multiplication between two tensors\n",
    "mat1 = th.randn(2, 3)\n",
    "mat2 = th.randn(3, 4)\n",
    "r = th.mm(mat1, mat2)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = th.tensor([2, 3])\n",
    "mat2 = th.tensor([2, 1])\n",
    "r = th.dot(mat1, mat2)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "![](https://colah.github.io/posts/2015-08-Backprop/img/tree-eval-derivs.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.tensor([1.], requires_grad=True)\n",
    "b = th.tensor([2.], requires_grad=True)\n",
    "\n",
    "c = a * b\n",
    "c.backward()\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = th.tensor([1.], requires_grad=True)\n",
    "b = th.tensor([2.], requires_grad=False)\n",
    "\n",
    "c = a*b\n",
    "c.backward()\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't care at all about the number of leaves in our computational graph. They can be as many as possible\n",
    "a_1 = th.tensor([1., 2., 3.], requires_grad=True)\n",
    "b_1 = th.tensor([4., 5., 6.], requires_grad=True)\n",
    "\n",
    "c_1 = (a_1 + b_1).sum()\n",
    "\n",
    "# be careful to compute the gradients only once per graph\n",
    "c_1.backward()\n",
    "print(a_1.grad)\n",
    "print(b_1.grad)\n",
    "\n",
    "a_2 = th.tensor([1., 2., 3.], requires_grad=True)\n",
    "b_2 = th.tensor([4., 5., 6.], requires_grad=True)\n",
    "c_2 = c_1 + (a_2 + b_2).sum()\n",
    "\n",
    "c_2.backward()\n",
    "\n",
    "print(a_1.grad)\n",
    "print(b_1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_1 = th.tensor([1., 2., 3.], requires_grad=True)\n",
    "b_1 = th.tensor([4., 5., 6.], requires_grad=True)\n",
    "\n",
    "print(a_1.requires_grad)\n",
    "print(b_1.requires_grad)\n",
    "\n",
    "with th.no_grad():\n",
    "    c_1 = a_1 + b_1\n",
    "    print(c_1.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Links:\n",
    "* [PyTorch](https://pytorch.org/docs/stable/) docs are amazing!\n",
    "* [The blog of Chris Olah@OpenAI](https://colah.github.io/posts/2015-08-Backprop/) made me understand backpropagation and LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
